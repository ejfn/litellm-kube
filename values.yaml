# LiteLLM Helm Chart Values
# Configure your LiteLLM deployment here

replicaCount: 1

image:
  repository: ghcr.io/berriai/litellm
  tag: main-stable
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 4000

ingress:
  enabled: false

proxy_config:
  # A list of models to preload into the LiteLLM proxy. Each entry should
  # use model_name and litellm_params format as documented at:
  # https://docs.litellm.ai/docs/proxy/configs
  model_list:
    # Anthropic Models - ensures Claude Code builtin models continue working
    - model_name: claude-*
      litellm_params:
        model: anthropic/claude-*
        api_key: os.environ/ANTHROPIC_API_KEY
        drop_params: true
    # OpenAI Models
    - model_name: gpt-*
      litellm_params:
        model: openai/gpt-*
        api_key: os.environ/OPENAI_API_KEY
        drop_params: true        
    # Google Gemini Models
    - model_name: gemini-*
      litellm_params:
        model: gemini/gemini-*
        api_key: os.environ/GOOGLE_API_KEY
        drop_params: true
    # Wildcard to access any OpenRouter model
    - model_name: "*"
      litellm_params:
        model: openrouter/*
        api_key: os.environ/OPENROUTER_API_KEY
        drop_params: true
        
# A list of Kubernetes Secret objects that will be exported to the LiteLLM proxy
#  pod as environment variables.  These secrets can then be referenced in the
#  configuration file (or "litellm" ConfigMap) with `os.environ/<Env Var Name>`
environmentSecrets:
  - litellm-provider-keys

# Resources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# Auto-scaling (optional)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
